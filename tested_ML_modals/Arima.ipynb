{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install hopsworks[python] statsmodels requests matplotlib pandas numpy scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bsrlLlv-70GK",
        "outputId": "5a80aebc-a10f-4b15-9d36-f8351177366d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting hopsworks[python]\n",
            "  Downloading hopsworks-4.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pyhumps==1.6.1 (from hopsworks[python])\n",
            "  Downloading pyhumps-1.6.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting furl (from hopsworks[python])\n",
            "  Downloading furl-2.1.4-py2.py3-none-any.whl.metadata (25 kB)\n",
            "Collecting boto3 (from hopsworks[python])\n",
            "  Downloading boto3-1.40.11-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjks (from hopsworks[python])\n",
            "  Downloading pyjks-20.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mock (from hopsworks[python])\n",
            "  Downloading mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting avro==1.11.3 (from hopsworks[python])\n",
            "  Downloading avro-1.11.3.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyMySQL[rsa] (from hopsworks[python])\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from hopsworks[python]) (5.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from hopsworks[python]) (2025.3.0)\n",
            "Collecting retrying (from hopsworks[python])\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting hopsworks_aiomysql==0.2.1 (from hopsworks_aiomysql[sa]==0.2.1->hopsworks[python])\n",
            "  Downloading hopsworks_aiomysql-0.2.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting opensearch-py<=2.4.2,>=1.1.0 (from hopsworks[python])\n",
            "  Downloading opensearch_py-2.4.2-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from hopsworks[python]) (4.67.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.49.1 in /usr/local/lib/python3.11/dist-packages (from hopsworks[python]) (1.74.0)\n",
            "Collecting protobuf<5.0.0,>=4.25.4 (from hopsworks[python])\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hopsworks[python]) (25.0)\n",
            "Requirement already satisfied: pyarrow>=10.0 in /usr/local/lib/python3.11/dist-packages (from hopsworks[python]) (18.1.0)\n",
            "Collecting confluent-kafka<=2.6.1 (from hopsworks[python])\n",
            "  Downloading confluent_kafka-2.6.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<=1.8.4,>=1.4.11 (from hopsworks[python])\n",
            "  Downloading fastavro-1.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting sqlalchemy<=2.0.29,>=1.3 (from hopsworks_aiomysql[sa]==0.2.1->hopsworks[python])\n",
            "  Downloading SQLAlchemy-2.0.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.16.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks[python]) (1.17.0)\n",
            "Collecting botocore<1.41.0,>=1.40.11 (from boto3->hopsworks[python])\n",
            "  Downloading botocore-1.40.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->hopsworks[python])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->hopsworks[python])\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting orderedmultidict>=1.0.1 (from furl->hopsworks[python])\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting javaobj-py3 (from pyjks->hopsworks[python])\n",
            "  Downloading javaobj_py3-0.4.4-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pyasn1>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from pyjks->hopsworks[python]) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from pyjks->hopsworks[python]) (0.4.2)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.11/dist-packages (from pyjks->hopsworks[python]) (3.23.0)\n",
            "Collecting twofish (from pyjks->hopsworks[python])\n",
            "  Downloading twofish-0.3.0.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from PyMySQL[rsa]->hopsworks[python]) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<=2.0.29,>=1.3->hopsworks_aiomysql[sa]==0.2.1->hopsworks[python]) (4.14.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<=2.0.29,>=1.3->hopsworks_aiomysql[sa]==0.2.1->hopsworks[python]) (3.2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->PyMySQL[rsa]->hopsworks[python]) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->PyMySQL[rsa]->hopsworks[python]) (2.22)\n",
            "Downloading hopsworks_aiomysql-0.2.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confluent_kafka-2.6.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opensearch_py-2.4.2-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.6/258.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.11-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading furl-2.1.4-py2.py3-none-any.whl (27 kB)\n",
            "Downloading hopsworks-4.3.1-py3-none-any.whl (670 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mock-5.2.0-py3-none-any.whl (31 kB)\n",
            "Downloading pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading botocore-1.40.11-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading javaobj_py3-0.4.4-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: avro, twofish\n",
            "  Building wheel for avro (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro: filename=avro-1.11.3-py2.py3-none-any.whl size=123962 sha256=4adcff20999d1c92f319d28c7efa83757637831cb21ea5e33abad108da43c73d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/7c/a4/fa31e47be300f6e6036f57769474de0ba54f8c6e8e2d8b7547\n",
            "  Building wheel for twofish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twofish: filename=twofish-0.3.0-cp311-cp311-linux_x86_64.whl size=24233 sha256=cefd25e5a864fd079e019d140a7155280e84c325d5b0e4d5a0d671474a2d5efb\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/0b/b1/d97875c8e719f4a31f39c3ea718798318be32cf0068b042351\n",
            "Successfully built avro twofish\n",
            "Installing collected packages: twofish, pyhumps, javaobj-py3, sqlalchemy, retrying, PyMySQL, protobuf, orderedmultidict, numpy, mock, jmespath, fastavro, confluent-kafka, avro, pyjks, opensearch-py, hopsworks_aiomysql, furl, botocore, s3transfer, boto3, hopsworks\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.43\n",
            "    Uninstalling SQLAlchemy-2.0.43:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.43\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMySQL-1.1.1 avro-1.11.3 boto3-1.40.11 botocore-1.40.11 confluent-kafka-2.6.1 fastavro-1.8.4 furl-2.1.4 hopsworks-4.3.1 hopsworks_aiomysql-0.2.1 javaobj-py3-0.4.4 jmespath-1.0.1 mock-5.2.0 numpy-1.26.4 opensearch-py-2.4.2 orderedmultidict-1.0.1 protobuf-4.25.8 pyhumps-1.6.1 pyjks-20.0.0 retrying-1.4.2 s3transfer-0.13.1 sqlalchemy-2.0.29 twofish-0.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "fd5c081a7a364507bda0a30e81a8b261"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import hopsworks\n",
        "\n",
        "FEATURE_GROUP_NAME = \"aqi_weather_features\"\n",
        "FEATURE_GROUP_VER  = 2\n",
        "LATITUDE  = 33.5973\n",
        "LONGITUDE = 73.0479\n",
        "HORIZON_H = 72\n",
        "TZ = \"Asia/Karachi\"\n",
        "ARTIFACT_DIR = \"arima_aqi_artifacts\"\n",
        "PLOTS_DIR    = os.path.join(ARTIFACT_DIR, \"plots\")\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# 1) Load data\n",
        "# ------------------------\n",
        "print(\"[1/6] Loading Feature Group from Hopsworks...\")\n",
        "project = hopsworks.login()\n",
        "fs = project.get_feature_store()\n",
        "fg = fs.get_feature_group(name=FEATURE_GROUP_NAME, version=FEATURE_GROUP_VER)\n",
        "df_raw = fg.read()\n",
        "\n",
        "df_raw = df_raw.sort_values(\"time\", ascending=True).reset_index(drop=True)\n",
        "df_raw[\"time\"] = pd.to_datetime(df_raw[\"time\"]).dt.tz_localize(None)\n",
        "\n",
        "# ------------------------\n",
        "# 2) Train ARIMA\n",
        "# ------------------------\n",
        "print(\"[2/6] Training ARIMA model...\")\n",
        "y = df_raw[\"us_aqi\"].values\n",
        "# Example ARIMA order — can be tuned via AIC/BIC\n",
        "model = ARIMA(y, order=(5,1,2))\n",
        "model_fit = model.fit()\n",
        "\n",
        "# ------------------------\n",
        "# 3) Forecast\n",
        "# ------------------------\n",
        "print(\"[3/6] Forecasting next 72h...\")\n",
        "forecast = model_fit.forecast(steps=HORIZON_H)\n",
        "last_time = df_raw[\"time\"].iloc[-1]\n",
        "future_times = pd.date_range(start=last_time + pd.Timedelta(hours=1), periods=HORIZON_H, freq=\"H\", tz=TZ)\n",
        "\n",
        "forecast_df = pd.DataFrame({\n",
        "    \"datetime\": future_times,\n",
        "    \"predicted_us_aqi\": forecast\n",
        "})\n",
        "forecast_path = os.path.join(ARTIFACT_DIR, \"arima_72h_forecast.csv\")\n",
        "forecast_df.to_csv(forecast_path, index=False)\n",
        "\n",
        "# ------------------------\n",
        "# 4) Plot\n",
        "# ------------------------\n",
        "print(\"[4/6] Saving forecast plot...\")\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df_raw[\"time\"].tail(200), df_raw[\"us_aqi\"].tail(200), label=\"History\")\n",
        "plt.plot(forecast_df[\"datetime\"], forecast_df[\"predicted_us_aqi\"], label=\"ARIMA Forecast\", linestyle=\"--\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(PLOTS_DIR, \"arima_forecast.png\"), dpi=140)\n",
        "plt.close()\n",
        "\n",
        "# ------------------------\n",
        "# 5) Evaluate (last 20% of history)\n",
        "# ------------------------\n",
        "print(\"[5/6] Evaluating ARIMA on last 20% history...\")\n",
        "split_idx = int(len(y) * 0.8)\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "model_val = ARIMA(y_train, order=(5,1,2)).fit()\n",
        "pred_test = model_val.forecast(steps=len(y_test))\n",
        "mae = mean_absolute_error(y_test, pred_test)\n",
        "rmse = mean_squared_error(y_test, pred_test)\n",
        "r2 = r2_score(y_test, pred_test)\n",
        "print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}\")\n",
        "\n",
        "print(\"\\n✅ ARIMA pipeline done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-M5CdbR7spn",
        "outputId": "0826c004-21f2-4ec1-eb10-ae1febbecfa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/6] Loading Feature Group from Hopsworks...\n",
            "Connection closed.\n",
            "Copy your Api Key (first register/login): https://c.app.hopsworks.ai/account/api/generated\n",
            "\n",
            "Paste it here: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1239199\n",
            "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.60s) \n",
            "[2/6] Training ARIMA model...\n",
            "[3/6] Forecasting next 72h...\n",
            "[4/6] Saving forecast plot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5/6] Evaluating ARIMA on last 20% history...\n",
            "MAE: 30.09, RMSE: 1544.45, R²: -0.2713\n",
            "\n",
            "✅ ARIMA pipeline done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF3yczq67lY5",
        "outputId": "f3960ba4-b1b7-4bab-bb14-f9c0182c6ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ARIMA Forecast ===\n",
            "              datetime        mean\n",
            "0  2025-08-15 00:00:00   97.358120\n",
            "1  2025-08-15 01:00:00  100.567657\n",
            "2  2025-08-15 02:00:00  103.138915\n",
            "3  2025-08-15 03:00:00  105.055568\n",
            "4  2025-08-15 04:00:00  106.229798\n",
            "..                 ...         ...\n",
            "67 2025-08-17 19:00:00  105.713944\n",
            "68 2025-08-17 20:00:00  105.697435\n",
            "69 2025-08-17 21:00:00  105.713804\n",
            "70 2025-08-17 22:00:00  105.697574\n",
            "71 2025-08-17 23:00:00  105.713666\n",
            "\n",
            "[72 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# arima_forecast.py\n",
        "import requests\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# # === Load trained ARIMA model ===\n",
        "# MODEL_PATH = \"arima_model.pkl\"\n",
        "# model = joblib.load(MODEL_PATH)\n",
        "\n",
        "# === Config ===\n",
        "lat, lon = 33.6844, 73.0479  # Example: Islamabad\n",
        "forecast_hours = 72\n",
        "\n",
        "# === Step 1: Fetch AQI Forecast from Open-Meteo ===\n",
        "aqi_url = (\n",
        "    \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
        "    f\"?latitude={lat}&longitude={lon}&hourly=us_aqi\"\n",
        ")\n",
        "\n",
        "aqi_data = requests.get(aqi_url).json()\n",
        "df_aqi = pd.DataFrame({\n",
        "    \"datetime\": aqi_data[\"hourly\"][\"time\"],\n",
        "    \"aqi\": aqi_data[\"hourly\"][\"us_aqi\"]\n",
        "})\n",
        "\n",
        "# Convert datetime to pandas datetime\n",
        "df_aqi[\"datetime\"] = pd.to_datetime(df_aqi[\"datetime\"])\n",
        "df_aqi.set_index(\"datetime\", inplace=True)\n",
        "\n",
        "# === Step 2: Forecast with ARIMA ===\n",
        "# Use the fitted model object (model_fit) and the forecast method\n",
        "forecast = model_fit.forecast(steps=forecast_hours)\n",
        "# The forecast method returns a numpy array, so create a DataFrame manually\n",
        "pred_df = pd.DataFrame({\"mean\": forecast})\n",
        "\n",
        "\n",
        "# === Step 3: Combine timestamps with predictions ===\n",
        "# Use the timestamps from the fetched aqi data for the forecast period\n",
        "pred_df[\"datetime\"] = df_aqi.index[:forecast_hours].values\n",
        "pred_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"\\n=== ARIMA Forecast ===\")\n",
        "print(pred_df[[\"datetime\", \"mean\"]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3GR9zaze81Sw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}